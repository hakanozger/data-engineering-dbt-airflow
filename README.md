# ğŸš€ Data Engineering Pipeline with Flask, PostgreSQL, dbt & Airflow

## ğŸ“Œ Overview

This project is a containerized data pipeline that:

1. **ğŸ“¡ Generates Data via a Flask API**: A Flask application simulates data generation and provides endpoints for data ingestion.
2. **ğŸ—„ï¸ Stores Data in PostgreSQL**: Ingested data is stored in a PostgreSQL database for persistence.
3. **ğŸ”„ Transforms Data Using dbt**: Data Build Tool (dbt) is utilized for data modeling and transformation, converting raw data into a structured format.
4. **ğŸ“Š Automates Workflows with Apache Airflow**: Airflow orchestrates the data pipeline, scheduling and managing the ETL processes.

## ğŸ› ï¸ Tech Stack

- **ğŸ³ Docker**: Containerization and orchestration
- **ğŸŒ Flask**: API for data ingestion
- **ğŸ˜ PostgreSQL**: Database for storing raw and transformed data
- **âš¡ dbt**: Data modeling and transformation
- **â³ Apache Airflow**: Workflow orchestration

## ğŸ“‚ Project Structure

The repository is organized as follows:

```
data-engineering-project/
â”œâ”€â”€ âš¡ airflow/                # Airflow DAGs and setup
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ dbt_dag.py      # Airflow DAG for dbt
â”‚   â”œâ”€â”€ Dockerfile          # Airflow Dockerfile
â”œâ”€â”€ ğŸ“Š dbt/                    # dbt project
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_payment_amount.sql
â”‚   â”‚   â”œâ”€â”€ transformations/
â”‚   â”‚   â”‚   â”œâ”€â”€ payment_amount.sql
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ profiles.yml        # dbt profiles configuration
â”œâ”€â”€ ğŸŒ flask-api/              # Flask application
â”‚   â”œâ”€â”€ app.py              # Main Flask app
â”‚   â”œâ”€â”€ db_utils.py         # Database utility functions
â”‚   â”œâ”€â”€ requirements.txt    # Flask dependencies
â”œâ”€â”€ ğŸ˜ postgres/               # PostgreSQL initialization
â”‚   â”œâ”€â”€ init.sql            # Database initialization script
â”œâ”€â”€ âš™ï¸ .env                    # Environment variables
â”œâ”€â”€ ğŸ“– README.md               # Project documentation
â”œâ”€â”€ ğŸ³ docker-compose.yml      # Docker Compose configuration
```

## âš™ï¸ Setup and Deployment

### ğŸ› ï¸ Prerequisites

- ğŸ³ Docker & Docker Compose installed
- ğŸ Python 3 installed

### ğŸš€ Steps to Run the Project

1. **ğŸ“¥ Clone the Repository**:
   ```bash
   git clone https://github.com/hakanozger/data-engineering-dbt-airflow.git
   cd data-engineering-dbt-airflow
   ```

2. **ğŸ“ Set Up Environment Variables**:
   - Create a `.env` file in the root directory with the necessary environment variables.
   - Ensure that the `profiles.yml` file is correctly configured for dbt.

3. **ğŸ—ï¸ Build and Start Docker Containers**:
   ```bash
   docker-compose up --build
   ```

### ğŸ” Verifying Setup
To check if dbt profile is properly mapped inside the container, run:
```sh
docker exec -it dbt ls /root/.dbt/
```
If `profiles.yml` is missing, manually copy it:
```sh
docker cp ./dbt/profiles.yml dbt:/root/.dbt/profiles.yml
```

### ğŸŒ Accessing Components

- **ğŸŒ Flask API**: Accessible at `http://localhost:5001`
- **ğŸ“Š Airflow UI**: Accessible at `http://localhost:8080` username=admin password: admin

## ğŸš€ Future Enhancements

- ğŸ“ˆ Implement logging and monitoring for better observability.
- âœ… Add data validation and quality checks to ensure data integrity.
- âš¡ Optimize dbt models for improved performance.

## ğŸ“œ License

This project is open-source and available for modifications.

For more details, please refer to the [GitHub repository](https://github.com/hakanozger/data-engineering-dbt-airflow).
